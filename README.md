
# ðŸ“ Smart AI Summarizer

## ðŸ“œ Project Overview
The **Smart AI Summarizer** is a web application that allows users to extract content from any website and generate a concise, summarized version of the content using **AI (Groq API)**. This project uses **Scrapy (for web scraping)**, **Flask (for API integration)**, and **React (for the frontend interface)** to deliver fast and accurate content summaries.

---

## ðŸš€ Features
- âœ… **Web scraping**: Extracts content (title and paragraphs) from any webpage using **Scrapy**.
- âœ… **AI-generated summary**: Uses **Groq API** to generate a summarized version of the content.
- âœ… **Copy to clipboard**: Allows users to copy the summary with a single click.
- âœ… **Download summary**: Provides an option to download the summary as a `.txt` file.
- âœ… **Real-time processing**: Ensures quick summarization using multi-threading in Flask.
- âœ… **Cross-browser support**: Can summarize any page the user visits (by clicking the summarize button).

---

## ðŸ“‚ Project Structure
The project contains three main components:

```
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”‚   â”œâ”€â”€ my_spider.py
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ smartsummarizer.jsx
```

---

## ðŸ•¸ Tech Stack
| Technology         | Purpose                                                               |
|-------------------|-------------------------------------------------------------------------|
| **Scrapy**         | Used for web scraping to extract content from web pages.              |
| **Scrapy-Selenium**| Enables dynamic content scraping using Selenium inside Scrapy.         |
| **Flask**          | Backend API to trigger the Scrapy spider and generate summaries.      |
| **React.js**       | Frontend interface for users to interact with the summarizer.        |
| **Groq API**       | AI model to summarize the content generated by Scrapy.               |
| **Flask-CORS**     | Handles cross-origin requests between frontend and backend.          |

---

## ðŸ“œ How It Works
### âœ… Step 1: Extract Content Using Scrapy
- When the user clicks **Summarize**, the frontend sends the **current tab URL** to the Flask API.
- The Flask API triggers the **Scrapy spider** (`my_spider.py`) to extract:
  - **Title of the page**
  - **All paragraph content** from the page

---

### âœ… Step 2: Generate Summary Using Groq API
- The scraped content is sent to the **Groq API (AI model)** with a request to generate a summary.
- The API is configured to return concise and key-point-based summaries.

---

### âœ… Step 3: Display, Copy, or Download Summary
- The summarized content is displayed to the user.
- The user has the option to:
  - âœ… **Copy to clipboard** - Copies the summary text.
  - âœ… **Download summary** - Downloads the summary as a `.txt` file.

---

## ðŸ“œ Installation and Setup
Follow these steps to set up and run the project.

### ðŸ“¥ Clone the Repository
```bash
git clone https://github.com/your-username/SmartAI-Summarizer.git
cd SmartAI-Summarizer
```

---

### âœ… 1. Setup Backend (Flask + Scrapy)
Navigate to the backend folder:
```bash
cd backend
```

Install dependencies:
```bash
pip install -r requirements.txt
```

Run the Flask server:
```bash
python app.py
```
The server will start at:
```
http://localhost:8000
```

---

### âœ… 2. Setup Frontend (React.js)
Navigate to the frontend folder:
```bash
cd frontend
```

Install dependencies:
```bash
npm install
```

Run the React app:
```bash
npm start
```
The frontend will open at:
```
http://localhost:3000
```

---

### âœ… 3. Chrome Extension Setup (Optional)
If you want to integrate it directly into the browser, follow these steps:
1. Build the React app:
```bash
npm run build
```
2. Open Chrome -> Extensions -> Enable Developer Mode.
3. Click **Load Unpacked** and select the `build` folder.
4. Now you can summarize any web page using the extension.

---

## ðŸ’» API Endpoints
The backend exposes the following API:

### ðŸš€ **POST** `/scrape`
**Description:** This endpoint triggers the Scrapy spider and summarizes content.

**Request Body:**
```json
{
  "url": "https://example.com/article"
}
```

**Response:**
```json
{
  "message": "Scraping completed successfully.",
  "data": [
    {
      "title": "Sample Title",
      "content": "Extracted content from the page."
    }
  ],
  "summary": "AI summarized content in short."
}
```

---

## ðŸ“œ Code Explanation
### ðŸ“œ 1. `my_spider.py` (Scrapy Spider)
- This file handles web scraping using Scrapy and Selenium.
- Extracts:
  - **Title** (`title::text`)
  - **Content** (`p::text`)
- Sends extracted data to Flask.

---

### ðŸ“œ 2. `app.py` (Flask API)
- Handles requests from the frontend.
- Runs the Scrapy spider using `CrawlerProcess`.
- Sends extracted data to **Groq API** for summarization.

---

### ðŸ“œ 3. `smartsummarizer.jsx` (React Component)
- Handles:
  - Sending the current URL to the backend.
  - Displaying the summary.
  - Copy and download functionality.

---

## ðŸ’Ž Environment Variables
Create a `.env` file in the `backend` directory and add:
```
GROQ_API_KEY=your-api-key-here
```

---

## ðŸ’¸ API Rate Limits
- The Groq API has a rate limit based on your plan.
- If you exhaust the limit, the summary may fail.

---

## ðŸ“Š Future Improvements
- âœ… Add multilingual support for summarizing content in different languages.
- âœ… Use OpenAI or Huggingface models for offline summarization.
- âœ… Improve content extraction by handling JavaScript-heavy websites.
- âœ… Add text-to-speech conversion for summary.

---

## ðŸ’¡ Credits
- **Developer:** [Your Name]
- **GitHub Repo:** [Your Repo Link]
- **API Used:** [Groq API](https://groq.com)
- **Web Scraping:** Scrapy + Selenium
- **Frontend:** React.js + Tailwind CSS
- **Backend:** Flask + Scrapy

---


ðŸ’¡ **Enjoy summarizing the web in seconds! ðŸš€**  
Would you like me to generate the browser extension **manifest.json** or a deployment script? ðŸš€
